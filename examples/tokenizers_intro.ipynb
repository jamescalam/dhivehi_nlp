{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Tokenizers\n",
    "\n",
    "First let us get an understanding of how each tokenizer works. Much of the inspiration here is thanks to the excellent [HuggingFace course on tokenizers](https://huggingface.co/course/chapter6/1?fw=pt).\n",
    "\n",
    "We will tokenize the following sentence using a simplified version of the three most common tokenizers:\n",
    "\n",
    "* BPE\n",
    "* WordPiece\n",
    "* Unigram (used in SentencePiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world hey would you help with my work'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello world! Hey would you help with my work?\"\n",
    "text = text.lower().replace('!', '').replace('?', '')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both BPE and WordPiece start by breaking our text into character-level tokens, but they do differ slightly, BPE is the simplest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'h', 'e', 'y', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'y', 'o', 'u', ' ', 'h', 'e', 'l', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'm', 'y', ' ', 'w', 'o', 'r', 'k']\n"
     ]
    }
   ],
   "source": [
    "#text = ' ' + text + ' '  # adding padding around the text makes life easier later\n",
    "bpe_text = list(text)\n",
    "print(bpe_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece identifies start-of-word tokens the same as BPE, but if a token is preceeded by another (non space character) token, it is a *part-of-word* token and prefixed with `##`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', '##e', '##l', '##l', '##o', ' ', 'w', '##o', '##r', '##l', '##d', ' ', 'h', '##e', '##y', ' ', 'w', '##o', '##u', '##l', '##d', ' ', 'y', '##o', '##u', ' ', 'h', '##e', '##l', '##p', ' ', 'w', '##i', '##t', '##h', ' ', 'm', '##y', ' ', 'w', '##o', '##r', '##k']\n"
     ]
    }
   ],
   "source": [
    "wordpiece_text = []\n",
    "for i, char in enumerate(text):\n",
    "    if i == 0 or text[i-1] == ' ' or char == ' ':\n",
    "        wordpiece_text.append(char)\n",
    "    else:\n",
    "        wordpiece_text.append('##'+char)\n",
    "print(wordpiece_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After breaking our text into these character-level tokens, we can create our *base vocabulary*, which is the initial starting vocab of our tokenizer. From this initial base vocab, all other tokens will be built by merging 'common' pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d', 'h', 'o', 'y', 'u', 't', 'l', 'r', ' ', 'k', 'i', 'm', 'e', 'w', 'p'} (len = 15)\n",
      "{'y', 'h', '##l', '##o', '##p', '##t', '##k', ' ', '##i', '##r', '##h', 'm', '##e', '##d', '##u', '##y', 'w'} (len = 17)\n"
     ]
    }
   ],
   "source": [
    "bpe_vocab = set(bpe_text)\n",
    "wordpiece_vocab = set(wordpiece_text)\n",
    "\n",
    "print(f\"{bpe_vocab} (len = {len(bpe_vocab)})\\n{wordpiece_vocab} (len = {len(wordpiece_vocab)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these vocabs we calculate the frequency of each token pair in the corpus. These frequencies identify which pairs to merge. The formula for deciding varies between BPE and WordPiece. We'll lead with the simpler BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_freq(corpus):\n",
    "    freq = {}\n",
    "    for i in range(1, len(corpus)):\n",
    "        one = corpus[i-1]\n",
    "        two = corpus[i]\n",
    "        if ' ' not in [one, two]:\n",
    "            pair = (one, two)\n",
    "            if pair not in freq.keys():\n",
    "                freq[pair] = 0\n",
    "            freq[pair] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('h', 'e'): 3, ('e', 'l'): 2, ('l', 'l'): 1, ('l', 'o'): 1, ('w', 'o'): 3, ('o', 'r'): 2, ('r', 'l'): 1, ('l', 'd'): 2, ('e', 'y'): 1, ('o', 'u'): 2, ('u', 'l'): 1, ('y', 'o'): 1, ('l', 'p'): 1, ('w', 'i'): 1, ('i', 't'): 1, ('t', 'h'): 1, ('m', 'y'): 1, ('r', 'k'): 1}\n"
     ]
    }
   ],
   "source": [
    "bpe_counts = bpe_freq(bpe_text)\n",
    "print(bpe_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BPE all we do now is merge the most common pair, in this case both `'he'` and `'wo'` are tied for first place, we will choose the first that appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'e']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = 0\n",
    "merge = ()\n",
    "for pair in bpe_counts.keys():\n",
    "    if bpe_counts[pair] > best:\n",
    "        merge = list(pair)\n",
    "        best = bpe_counts[pair]\n",
    "merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we go ahead and merge any instances of this pair in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'he', 'y', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'y', 'o', 'u', ' ', 'he', 'l', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'm', 'y', ' ', 'w', 'o', 'r', 'k']\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(bpe_text)):\n",
    "    if bpe_text[i-1:i+1] == merge:\n",
    "        # TODO merge the two in the list\n",
    "        bpe_text = bpe_text[:i-1] + [''.join(bpe_text[i-1:i+1])] + bpe_text[i+1:]\n",
    "print(bpe_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just repeat until reaching a specific vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he, l, l, o,  , wo, r, l, d,  , he, y,  , wo, u, l, d,  , y, o, u,  , he, l, p,  , w, i, t, h,  , m, y,  , wo, r, k\n",
      "hel, l, o,  , wo, r, l, d,  , he, y,  , wo, u, l, d,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wo, r, k\n",
      "hel, l, o,  , wor, l, d,  , he, y,  , wo, u, l, d,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hel, l, o,  , wor, ld,  , he, y,  , wo, u, ld,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hell, o,  , wor, ld,  , he, y,  , wo, u, ld,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hello,  , wor, ld,  , he, y,  , wo, u, ld,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hello,  , world,  , he, y,  , wo, u, ld,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hello,  , world,  , hey,  , wo, u, ld,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hello,  , world,  , hey,  , wou, ld,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hello,  , world,  , hey,  , would,  , y, o, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "hello,  , world,  , hey,  , would,  , yo, u,  , hel, p,  , w, i, t, h,  , m, y,  , wor, k\n",
      "vocab:\n",
      "{'y', 'o', 'yo', 'would', 'wor', 'hello', 'hell', 't', 'world', 'ld', 'u', 'r', 'm', 'e', 'wo', 'w', 'p', 'wou', 'd', 'h', 'hel', 'l', ' ', 'k', 'i', 'hey'}\n"
     ]
    }
   ],
   "source": [
    "while len(bpe_vocab) <= 25:\n",
    "    # get frequencies\n",
    "    bpe_counts = bpe_freq(bpe_text)\n",
    "    # get most common pair\n",
    "    best = 0\n",
    "    merge = ()\n",
    "    for pair in bpe_counts.keys():\n",
    "        if bpe_counts[pair] > best:\n",
    "            merge = list(pair)\n",
    "            best = bpe_counts[pair]\n",
    "    # then merge any instances of this pair in the corpus\n",
    "    for i in range(1, len(bpe_text)):\n",
    "        if bpe_text[i-1:i+1] == merge:\n",
    "            # TODO merge the two in the list\n",
    "            bpe_text = bpe_text[:i-1] + [''.join(bpe_text[i-1:i+1])] + bpe_text[i+1:]\n",
    "    # and add the new pair to the vocab\n",
    "    bpe_vocab.add(''.join(merge))\n",
    "    print(', '.join(bpe_text))\n",
    "print(f'vocab:\\n{bpe_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've build a BPE tokenizer, now let's apply it to a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'hello', 'world', 'hell', 'wor', 'wou', 'hel', 'hey', 'yo', 'ld', 'wo', 'y', 'o', 't', 'u', 'r', 'm', 'e', 'w', 'p', 'd', 'h', 'l', ' ', 'k', 'i']\n"
     ]
    }
   ],
   "source": [
    "bpe_vocab = sorted(list(bpe_vocab), key=len, reverse=True)\n",
    "print(bpe_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w, h, e, n,  , i,  , l, a, s, t,  , v, i, s, i, t, e, d,  , t, h, e,  , w, o, r, l, d,  , w, i, t, h,  , m, y,  , a, x, o, l, o, t, l,  , t, h, e, y,  , a, l, l,  , s, a, i, d,  , h, e, l, l, o\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(list(\"when i last visited the world with my axolotl they all said hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w, h, e, n,  , i,  , l, a, s, t,  , v, i, s, i, t, e, d,  , t, h, e,  , w, o, r, l, d,  , w, i, t, h,  , m, y,  , a, x, o, l, o, t, l,  , t, h, e, y,  , a, l, l,  , s, a, i, d,  , hello\n",
      "w, h, e, n,  , i,  , l, a, s, t,  , v, i, s, i, t, e, d,  , t, h, e,  , world,  , w, i, t, h,  , m, y,  , a, x, o, l, o, t, l,  , t, h, e, y,  , a, l, l,  , s, a, i, d,  , hello\n",
      "w, h, e, n,  , i,  , l, a, s, t,  , v, i, s, i, t, e, d,  , t, h, e,  , world,  , w, i, t, h,  , m, y,  , a, x, o, l, o, t, l,  , t, hey,  , a, l, l,  , s, a, i, d,  , hello\n",
      "['w', 'h', 'e', '<UNK>', ' ', 'i', ' ', 'l', '<UNK>', '<UNK>', 't', ' ', '<UNK>', 'i', '<UNK>', 'i', 't', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'world', ' ', 'w', 'i', 't', 'h', ' ', 'm', 'y', ' ', '<UNK>', '<UNK>', 'o', 'l', 'o', 't', 'l', ' ', 't', 'hey', ' ', '<UNK>', 'l', 'l', ' ', '<UNK>', '<UNK>', 'i', 'd', ' ', 'hello']\n"
     ]
    }
   ],
   "source": [
    "new_text = list(\"when i last visited the world with my axolotl they all said hello\")\n",
    "\n",
    "text_len = len(new_text)\n",
    "\n",
    "for token in bpe_vocab:\n",
    "    match = list(token)\n",
    "    token_len = len(match)\n",
    "    for i in range(len(new_text) - (token_len-1)):\n",
    "        if new_text[i:i+token_len] == match:\n",
    "            new_text = new_text[:i] + [token] + new_text[i+token_len:]\n",
    "    if text_len != len(new_text):\n",
    "        text_len = len(new_text)\n",
    "        print(', '.join(new_text))\n",
    "\n",
    "# do a final check and replace unknown tokens with '<UNK>'\n",
    "for i in range(len(new_text)):\n",
    "    if new_text[i] not in bpe_vocab:\n",
    "        new_text[i] = '<UNK>'\n",
    "\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how BPE tokenizers work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_freq = pair_freq(bpe_text)\n",
    "wordpiece_freq = pair_freq(wordpiece_text)\n",
    "\n",
    "print(f\"{bpe_freq}\\n{wordpiece_freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our counts, we use these frequencies to identify which pairs to merge. To formula for deciding varies between BPE and WordPiece. Again, we'll lead with BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dv-corpus-clean-unique.txt', 'r', encoding='utf-8') as fp:\n",
    "    dv = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = [pair.split('\\t')[1] for pair in dv if '\\t' in pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5188bc372fa413aa2565ae5d28228f50ad7b2c4ebb4a82c5900fd598adbb6408"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
