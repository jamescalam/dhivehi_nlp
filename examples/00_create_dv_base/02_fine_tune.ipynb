{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, filepath: str, tokenizer_name: str):\n",
    "        self.filepath = filepath\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_name)\n",
    "        self.t = {\n",
    "            'cls': self.tokenizer.cls_token_id,\n",
    "            'pad': self.tokenizer.pad_token_id,\n",
    "            'sep': self.tokenizer.sep_token_id,\n",
    "            'unk': self.tokenizer.unk_token_id,\n",
    "            'mask': self.tokenizer.mask_token_id\n",
    "        }\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        inputs = self.tokenizer(\n",
    "            text, max_length=512, padding='max_length',\n",
    "            truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        # clone the token IDs which will now be our target tokens\n",
    "        inputs['labels'] = inputs.input_ids.detach().clone()[0]\n",
    "        # mask input tokens\n",
    "        inputs['input_ids'] = self.mask(inputs['input_ids'])[0]\n",
    "        inputs['attention_mask'] = inputs['attention_mask'][0]\n",
    "        return inputs\n",
    "    \n",
    "    def mask(self, input_ids):\n",
    "        # create random array of floats with equal dimensions to input_ids tensor\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "        # create mask array\n",
    "        mask_arr = (rand < 0.15) * (input_ids != self.t['cls']) * \\\n",
    "            (input_ids != self.t['pad']) * (input_ids != self.t['sep']) * \\\n",
    "            (input_ids != self.t['unk'])\n",
    "        # take indices of each True value\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            input_ids[i, torch.flatten(mask_arr[i].nonzero()).tolist()] = self.tokenizer.mask_token_id\n",
    "        return input_ids\n",
    "    \n",
    "    def __iter__(self):\n",
    "        line = open(self.filepath, encoding='utf-8')\n",
    "        tokens = map(self.preprocess, line)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/dv-corpus-clean-unique-2m.txt'\n",
    "\n",
    "dataset = Dataset(train_file, 'bert-base-dv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=20_000,\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup CPU/GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_read = open(train_file, encoding='utf-8')\n",
    "num_samples = 0\n",
    "for row in dv_read:\n",
    "    num_samples += 1\n",
    "del row\n",
    "dv_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "model.train()\n",
    "optim = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# setup warmup for the first ~10% of steps\n",
    "total_steps = int(num_samples / batch_size) * epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptim, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = torch.utils.tensorboard.SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** when implementing NSP just shuffle the current dhivehi data into a new 'shuffled' file and pick non-following sentences from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True, total=int(num_samples/batch_size))\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # take loss for tensorboard\n",
    "        #writer.add_scalar('Loss/train', loss, step)\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        step += 1\n",
    "        # update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        # update the TDQM progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('bert-base-dv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ada91ca7be38ac141a70d8e06f4253d3e90604f2701bfa98443d880c4baa087"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('search': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
