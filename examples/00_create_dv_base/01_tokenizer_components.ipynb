{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a tokenizer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the normalization components, which are *convert to lowercase* -> *normalize with NFKC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Lowercase(), normalizers.NFKD()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have pretokenization, eg how do we split into words before tokenization? For Dhivehi we want to split on both whitespace and punctuation (a comma isn't part of a word - it's separate). This is a common approach and covered with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the tokenizer, we've already assigned the WordPiece tokenizer so all we do now is pass Dhivehi text data to the training function, specify any special tokens to include in our vocab (as they will not be found in the training data... Hopefully!), and specify our target vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=30_000,\n",
    "    special_tokens=['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]'],\n",
    "    min_frequency=2,\n",
    "    continuing_subword_prefix='##'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dv = open('../data/dv-corpus-clean-unique.txt', encoding='utf-8')\n",
    "tokenizer.train_from_iterator(read_dv, trainer=trainer)\n",
    "read_dv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_dv = open('../data/dv-corpus-clean-unique.txt', encoding='utf-8')\n",
    "count = 0\n",
    "with open('../data/dv-corpus-clean-unique-2m.txt', 'w', encoding='utf-8') as fp:\n",
    "    for row in read_dv:\n",
    "        fp.write(row+'\\n')\n",
    "        count += 1\n",
    "        if count > 2_000_000: break\n",
    "read_dv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the tokenizer we need to set the post-processing steps, eg add any special tokens. At the start and end of every sequence we add a classifier `[CLS]` token and a seperator `[SEP]` token. We will be adding these using their token IDs which we can find with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_id = tokenizer.token_to_id('[CLS]')\n",
    "sep_id = tokenizer.token_to_id('[SEP]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the post processing template we use something called the `TemplateProcessor`. This allows us to specify how to deal with both single sentences and sentence pairs (which we will need during NSP pretraining). The first/single sentence is represented by `$A` and the second (for pairs) is represented by `$B`.\n",
    "\n",
    "Our special tokens, `$A`, and `$B` are all followed by an integer value which indicated their token type ID value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f'[CLS]:0 $A:0 [SEP]:0',\n",
    "    pair=f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
    "    special_tokens=[\n",
    "        ('[CLS]', cls_id),\n",
    "        ('[SEP]', sep_id)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to set the `decoder` component of the tokenizer, here we just use the WordPiece tokenizer with the `##` word part prefix specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix='##')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer is now fully prepared, all that's left is to save it. To load the tokenizer with HF transformers we will first need to load our current tokenizer into a transformers fast tokenizer like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "full_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token='[UNK]',\n",
    "    pad_token='[PAD]',\n",
    "    cls_token='[CLS]',\n",
    "    sep_token='[SEP]',\n",
    "    mask_token='[MASK]'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Fast tokenizers are faster than the usual tokenizers because they are implemented in Rust)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-base-dv\\\\tokenizer_config.json',\n",
       " 'bert-base-dv\\\\special_tokens_map.json',\n",
       " 'bert-base-dv\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tokenizer.save_pretrained('bert-base-dv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('bert-base-dv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 23, 15160, 8745, 5062, 2979, 4520, 23, 26862, 24, 2560, 5244, 20410, 3033, 11341, 2994, 9595, 3082, 23200, 2801, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"(ސެންޓޭ)، ނާއިޒް ހަސަން (ދާދު) އަދި ހުސައިން ނިހާންގެ އިތުރުން ސަމްދޫހު މުހައްމަދު ހިމަނާފައިވެއެވެ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done, our tokenizer is ready."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5188bc372fa413aa2565ae5d28228f50ad7b2c4ebb4a82c5900fd598adbb6408"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
